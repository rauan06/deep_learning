{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models & Vision Robotics Practice Session\n",
    "\n",
    "**Total Points: 10 points**\n",
    "\n",
    "This notebook covers:\n",
    "- Part 1: Generative Models (5 points)\n",
    "  - Exercise 1.1: VAE Latent Space Analysis (2.5 points)\n",
    "  - Exercise 1.2: GAN Image Analysis (2.5 points)\n",
    "- Part 2: Vision and Robotics (5 points)\n",
    "  - Exercise 2.1: Robotic Manipulation Pipeline (2.5 points)\n",
    "  - Exercise 2.2: Semantic Segmentation for Navigation (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first)\n",
    "# !pip install torch torchvision matplotlib numpy opencv-python pillow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Generative Models (5 Points)\n",
    "\n",
    "## Exercise 1.1: Latent Space Analysis with VAE (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define Simple VAE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(SimpleVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 200),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(200, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(200, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Initialize VAE\n",
    "vae = SimpleVAE(latent_dim=20).to(device)\n",
    "print(\"VAE model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load MNIST Dataset and Train Simple VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Quick training function\n",
    "def train_vae(model, train_loader, epochs=3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            recon, mu, logvar = model(data)\n",
    "            \n",
    "            # Loss = Reconstruction + KL divergence\n",
    "            recon_loss = nn.functional.binary_cross_entropy(\n",
    "                recon, data.view(-1, 784), reduction='sum'\n",
    "            )\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = recon_loss + kl_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item()/len(data):.4f}')\n",
    "        \n",
    "        print(f'Epoch {epoch+1} completed, Avg Loss: {total_loss/len(train_loader.dataset):.4f}')\n",
    "\n",
    "# Train the model (you can skip this if you want to save time)\n",
    "print(\"Training VAE (this may take a few minutes)...\")\n",
    "train_vae(vae, train_loader, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Select Two Input Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get two different digit images\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Select two images (e.g., a '3' and a '7')\n",
    "img1 = test_dataset[10][0].to(device)  # First image\n",
    "img2 = test_dataset[25][0].to(device)  # Second image\n",
    "\n",
    "# Visualize original images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(img1.cpu().squeeze(), cmap='gray')\n",
    "axes[0].set_title('Image 1')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img2.cpu().squeeze(), cmap='gray')\n",
    "axes[1].set_title('Image 2')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vae_original_images.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Original images saved as 'vae_original_images.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Encode Images to Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    # Encode images\n",
    "    mu1, logvar1 = vae.encode(img1.view(1, -1))\n",
    "    mu2, logvar2 = vae.encode(img2.view(1, -1))\n",
    "    \n",
    "    # Use mean as latent representation\n",
    "    z1 = mu1\n",
    "    z2 = mu2\n",
    "\n",
    "print(f\"Latent representation z1 shape: {z1.shape}\")\n",
    "print(f\"Latent representation z2 shape: {z2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Linear Interpolation in Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear interpolation\n",
    "alpha_values = np.linspace(0, 1, 10)\n",
    "interpolated_images = []\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    for alpha in alpha_values:\n",
    "        # z(α) = (1 − α)z1 + αz2\n",
    "        z_interpolated = (1 - alpha) * z1 + alpha * z2\n",
    "        decoded = vae.decode(z_interpolated)\n",
    "        interpolated_images.append(decoded.cpu().view(28, 28).numpy())\n",
    "\n",
    "# Visualize interpolation\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (img, alpha) in enumerate(zip(interpolated_images, alpha_values)):\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'α = {alpha:.2f}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('vae_interpolation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpolation sequence saved as 'vae_interpolation.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Latent Space Exploration (Random Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random latent vectors from learned distribution\n",
    "random_samples = []\n",
    "\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(3):\n",
    "        # Sample from standard normal distribution\n",
    "        z_random = torch.randn(1, 20).to(device)\n",
    "        decoded = vae.decode(z_random)\n",
    "        random_samples.append(decoded.cpu().view(28, 28).numpy())\n",
    "\n",
    "# Visualize random samples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for i, img in enumerate(random_samples):\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'Random Sample {i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Latent Space Exploration - Random Sampling', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('vae_random_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Random samples saved as 'vae_random_samples.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Analysis of VAE Latent Space (150-200 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Latent Space Analysis:**\n",
    "\n",
    "Based on the interpolation and exploration experiments, the VAE latent space encodes several important semantic features of handwritten digits. The interpolation sequence demonstrates smooth transitions between different digit classes, indicating that the latent space is continuous and well-structured. The intermediate images show gradual morphing from one digit to another, suggesting that the model has learned to encode geometric properties like stroke thickness, curvature, and overall shape.\n",
    "\n",
    "The random sampling results reveal that the latent space captures the general characteristics of digit writing styles. Most randomly generated samples produce recognizable digit-like structures, though some may appear as blends or ambiguous forms. This indicates that the model has learned a probabilistic representation where similar digits cluster together in the latent space.\n",
    "\n",
    "Key semantic features encoded include: (1) digit identity and class membership, (2) writing style variations such as slant and thickness, (3) structural components like loops and strokes, and (4) overall digit proportions. The smooth interpolations suggest that the latent dimensions collectively represent these features in a distributed manner rather than having single dimensions dedicated to specific attributes. The continuity of the latent space enables meaningful arithmetic operations and controlled generation of digit variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 1.2: Analysis of GAN-Generated Images (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simple GAN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(SimpleGenerator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class SimpleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleDiscriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize GAN\n",
    "generator = SimpleGenerator(latent_dim=100).to(device)\n",
    "discriminator = SimpleDiscriminator().to(device)\n",
    "\n",
    "print(\"GAN models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train Simple GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, train_loader, epochs=3):\n",
    "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (real_images, _) in enumerate(train_loader):\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images = real_images.view(batch_size, -1).to(device)\n",
    "            \n",
    "            # Train Discriminator\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            d_optimizer.zero_grad()\n",
    "            outputs = discriminator(real_images)\n",
    "            d_loss_real = criterion(outputs, real_labels)\n",
    "            \n",
    "            z = torch.randn(batch_size, 100).to(device)\n",
    "            fake_images = generator(z)\n",
    "            outputs = discriminator(fake_images.detach())\n",
    "            d_loss_fake = criterion(outputs, fake_labels)\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # Train Generator\n",
    "            g_optimizer.zero_grad()\n",
    "            z = torch.randn(batch_size, 100).to(device)\n",
    "            fake_images = generator(z)\n",
    "            outputs = discriminator(fake_images)\n",
    "            g_loss = criterion(outputs, real_labels)\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}], '\n",
    "                      f'D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
    "\n",
    "# Train GAN\n",
    "print(\"Training GAN (this may take a few minutes)...\")\n",
    "train_gan(generator, discriminator, train_loader, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate Synthetic Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 10 fake images\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(10, 100).to(device)\n",
    "    fake_images = generator(z).cpu().view(-1, 28, 28).numpy()\n",
    "    # Convert from tanh output [-1, 1] to [0, 1]\n",
    "    fake_images = (fake_images + 1) / 2\n",
    "\n",
    "# Get 10 real images\n",
    "real_images = []\n",
    "for i in range(10):\n",
    "    real_images.append(test_dataset[i][0].squeeze().numpy())\n",
    "\n",
    "# Create mixed gallery\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Randomly mix real and fake\n",
    "all_images = [(img, 'Real') for img in real_images] + [(img, 'Generated') for img in fake_images]\n",
    "random.shuffle(all_images)\n",
    "\n",
    "for i, (img, label) in enumerate(all_images):\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'{label}', fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Mixed Gallery: Real vs GAN-Generated Images', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('gan_mixed_gallery.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Mixed gallery saved as 'gan_mixed_gallery.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Identify GAN Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Five Common GAN Artifacts Observed:**\n",
    "\n",
    "1. **Blurry Edges**: GAN-generated digits often have softer, less defined edges compared to real handwritten digits which have sharp ink boundaries.\n",
    "\n",
    "2. **Inconsistent Line Thickness**: Generated images may show unnatural variations in stroke width within a single digit, unlike consistent pen pressure in real writing.\n",
    "\n",
    "3. **Ghosting/Double Lines**: Some generated digits exhibit faint duplicate strokes or shadows, creating a ghosting effect not present in real images.\n",
    "\n",
    "4. **Unnatural Curves**: The curvature of digits may appear too smooth or mechanically perfect, lacking the natural irregularity of human handwriting.\n",
    "\n",
    "5. **Background Noise Patterns**: Generated images sometimes have subtle grid-like or repetitive noise patterns in the background, while real images have uniform backgrounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Latent Space Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5 images with different random vectors\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    z_samples = torch.randn(5, 100).to(device)\n",
    "    generated_samples = generator(z_samples).cpu().view(-1, 28, 28).numpy()\n",
    "    generated_samples = (generated_samples + 1) / 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, img in enumerate(generated_samples):\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'Sample {i+1}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Different Random Latent Vectors', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('gan_random_vectors.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Apply perturbations to one latent vector\n",
    "base_z = torch.randn(1, 100).to(device)\n",
    "perturbations = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Original\n",
    "    original = generator(base_z).cpu().view(28, 28).numpy()\n",
    "    perturbations.append((original, 'Original'))\n",
    "    \n",
    "    # Perturb different dimensions\n",
    "    for dim in [0, 10, 25, 50]:\n",
    "        z_perturbed = base_z.clone()\n",
    "        z_perturbed[0, dim] += 0.5  # Add perturbation\n",
    "        perturbed = generator(z_perturbed).cpu().view(28, 28).numpy()\n",
    "        perturbations.append((perturbed, f'Dim {dim} +0.5'))\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, (img, label) in enumerate(perturbations):\n",
    "    img = (img + 1) / 2\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Effects of Latent Vector Perturbations', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('gan_perturbations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Latent manipulation images saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: VAE vs GAN Comparative Analysis (150-200 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparative Analysis: VAE vs GAN Outputs**\n",
    "\n",
    "VAE and GAN outputs exhibit distinct characteristics in image quality, diversity, and artifacts. VAE-generated images tend to be blurrier and smoother due to the reconstruction loss that encourages pixel-wise similarity. This results in more conservative outputs that capture the average characteristics of the training data. VAEs produce images with less sharp details but maintain better overall structural coherence. The latent space in VAEs is continuous and well-structured, enabling reliable interpolation between images.\n",
    "\n",
    "In contrast, GAN-generated images typically show sharper details and more realistic textures because the adversarial training encourages outputs that fool the discriminator. However, GANs are prone to mode collapse and may produce repetitive patterns or specific artifacts like checkerboard patterns and unnatural color distributions. GANs can achieve higher perceptual quality but sometimes sacrifice diversity.\n",
    "\n",
    "Regarding artifacts, VAEs exhibit blurriness and lack of fine details as their primary weakness, while GANs show sharper but sometimes unrealistic features, inconsistent textures, and training instabilities. VAEs offer more predictable and stable training with interpretable latent spaces, whereas GANs provide superior visual quality at the cost of training difficulty and potential artifact generation. The choice between them depends on whether controllability and stability (VAE) or visual fidelity (GAN) is prioritized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Deep Learning for Vision and Robotics (5 Points)\n",
    "\n",
    "## Exercise 2.1: Vision-Based Robotic Manipulation Pipeline (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Simulated Workspace Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple simulated tabletop scene\n",
    "img_width, img_height = 800, 600\n",
    "workspace_img = np.ones((img_height, img_width, 3), dtype=np.uint8) * 220  # Light background\n",
    "\n",
    "# Simulate objects with bounding boxes (format: [x, y, w, h, label, confidence])\n",
    "# In a real scenario, these would come from a detection model like YOLO\n",
    "detected_objects = [\n",
    "    {'bbox': [100, 150, 120, 100], 'label': 'Coffee Mug', 'confidence': 0.95, 'distance': 30},\n",
    "    {'bbox': [350, 200, 80, 60], 'label': 'Pen', 'confidence': 0.89, 'distance': 25},\n",
    "    {'bbox': [550, 180, 150, 130], 'label': 'Notebook', 'confidence': 0.92, 'distance': 35},\n",
    "    {'bbox': [200, 380, 90, 70], 'label': 'Phone', 'confidence': 0.88, 'distance': 28},\n",
    "    {'bbox': [480, 400, 110, 90], 'label': 'Water Bottle', 'confidence': 0.91, 'distance': 32},\n",
    "]\n",
    "\n",
    "# Draw objects on workspace\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (255, 0, 255)]\n",
    "\n",
    "for i, obj in enumerate(detected_objects):\n",
    "    x, y, w, h = obj['bbox']\n",
    "    color = colors[i % len(colors)]\n",
    "    \n",
    "    # Draw filled rectangle for object\n",
    "    cv2.rectangle(workspace_img, (x, y), (x+w, y+h), color, -1)\n",
    "    \n",
    "    # Add some texture\n",
    "    cv2.rectangle(workspace_img, (x, y), (x+w, y+h), (0, 0, 0), 2)\n",
    "    \n",
    "    # Add label\n",
    "    cv2.putText(workspace_img, obj['label'], (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(cv2.cvtColor(workspace_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Simulated Tabletop Workspace')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('workspace_scene.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply Object Detection and Extract Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate image with bounding boxes and centroids\n",
    "annotated_img = workspace_img.copy()\n",
    "\n",
    "for obj in detected_objects:\n",
    "    x, y, w, h = obj['bbox']\n",
    "    \n",
    "    # Calculate centroid\n",
    "    centroid_x = x + w // 2\n",
    "    centroid_y = y + h // 2\n",
    "    obj['centroid'] = (centroid_x, centroid_y)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(annotated_img, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "    \n",
    "    # Draw centroid\n",
    "    cv2.circle(annotated_img, (centroid_x, centroid_y), 8, (255, 0, 0), -1)\n",
    "    \n",
    "    # Add info text\n",
    "    info_text = f\"{obj['label']}: {obj['confidence']:.2f}\"\n",
    "    cv2.putText(annotated_img, info_text, (x, y-10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "    cv2.putText(annotated_img, f\"({centroid_x}, {centroid_y})\", \n",
    "                (centroid_x+15, centroid_y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Object Detection with Bounding Boxes and Centroids')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('detection_annotated.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Annotated detection image saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Ranking Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate object areas and assign graspability scores\n",
    "graspability_scores = {\n",
    "    'Coffee Mug': 0.9,\n",
    "    'Pen': 0.7,\n",
    "    'Notebook': 0.6,\n",
    "    'Phone': 0.85,\n",
    "    'Water Bottle': 0.95\n",
    "}\n",
    "\n",
    "for obj in detected_objects:\n",
    "    x, y, w, h = obj['bbox']\n",
    "    obj['area'] = w * h\n",
    "    obj['graspability'] = graspability_scores.get(obj['label'], 0.5)\n",
    "    \n",
    "    # Calculate priority score (weighted combination)\n",
    "    # Higher confidence, closer distance, medium size, and high graspability = higher priority\n",
    "    confidence_weight = 0.3\n",
    "    distance_weight = 0.3\n",
    "    size_weight = 0.2\n",
    "    graspability_weight = 0.2\n",
    "    \n",
    "    # Normalize distance (closer = higher score)\n",
    "    distance_score = 1.0 - (obj['distance'] - 20) / 20  # Normalized to ~0-1\n",
    "    \n",
    "    # Normalize size (prefer medium-sized objects)\n",
    "    optimal_area = 10000\n",
    "    size_score = 1.0 - abs(obj['area'] - optimal_area) / optimal_area\n",
    "    size_score = max(0, min(1, size_score))\n",
    "    \n",
    "    obj['priority_score'] = (\n",
    "        confidence_weight * obj['confidence'] +\n",
    "        distance_weight * distance_score +\n",
    "        size_weight * size_score +\n",
    "        graspability_weight * obj['graspability']\n",
    "    )\n",
    "\n",
    "# Sort by priority score\n",
    "ranked_objects = sorted(detected_objects, key=lambda x: x['priority_score'], reverse=True)\n",
    "\n",
    "# Display ranking table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OBJECT PRIORITY RANKING FOR ROBOTIC MANIPULATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Rank':<6} {'Object':<15} {'Conf':<6} {'Dist(cm)':<10} {'Area(px²)':<10} {'Grasp':<7} {'Priority':<8}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for i, obj in enumerate(ranked_objects, 1):\n",
    "    print(f\"{i:<6} {obj['label']:<15} {obj['confidence']:<6.2f} {obj['distance']:<10} \"\n",
    "          f\"{obj['area']:<10} {obj['graspability']:<7.2f} {obj['priority_score']:<8.3f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nRecommended Pick Sequence: {' → '.join([obj['label'] for obj in ranked_objects])}\")\n",
    "print(\"\\nRanking Justification:\")\n",
    "print(\"- Confidence (30%): Higher detection confidence reduces grasp failure risk\")\n",
    "print(\"- Distance (30%): Closer objects require less arm movement and are faster to reach\")\n",
    "print(\"- Size (20%): Medium-sized objects are easier to grasp reliably\")\n",
    "print(\"- Graspability (20%): Object shape and material affect grasp success rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Visualize Pick Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization with numbered pick sequence\n",
    "sequence_img = workspace_img.copy()\n",
    "\n",
    "for i, obj in enumerate(ranked_objects, 1):\n",
    "    x, y, w, h = obj['bbox']\n",
    "    centroid_x, centroid_y = obj['centroid']\n",
    "    \n",
    "    # Color code by priority (green=high, yellow=medium, red=low)\n",
    "    if i <= 2:\n",
    "        color = (0, 255, 0)  # Green\n",
    "    elif i <= 4:\n",
    "        color = (0, 255, 255)  # Yellow\n",
    "    else:\n",
    "        color = (0, 0, 255)  # Red\n",
    "    \n",
    "    cv2.rectangle(sequence_img, (x, y), (x+w, y+h), color, 4)\n",
    "    \n",
    "    # Draw large priority number\n",
    "    cv2.circle(sequence_img, (centroid_x, centroid_y), 25, (255, 255, 255), -1)\n",
    "    cv2.circle(sequence_img, (centroid_x, centroid_y), 25, (0, 0, 0), 3)\n",
    "    cv2.putText(sequence_img, str(i), (centroid_x-12, centroid_y+12),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 0), 3)\n",
    "\n",
    "# Add legend\n",
    "cv2.rectangle(sequence_img, (10, 10), (250, 130), (255, 255, 255), -1)\n",
    "cv2.rectangle(sequence_img, (10, 10), (250, 130), (0, 0, 0), 2)\n",
    "cv2.putText(sequence_img, \"Pick Sequence:\", (20, 35),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "cv2.rectangle(sequence_img, (20, 45), (40, 60), (0, 255, 0), -1)\n",
    "cv2.putText(sequence_img, \"High Priority (1-2)\", (50, 58),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "cv2.rectangle(sequence_img, (20, 70), (40, 85), (0, 255, 255), -1)\n",
    "cv2.putText(sequence_img, \"Medium Priority (3-4)\", (50, 83),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "cv2.rectangle(sequence_img, (20, 95), (40, 110), (0, 0, 255), -1)\n",
    "cv2.putText(sequence_img, \"Low Priority (5)\", (50, 108),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(cv2.cvtColor(sequence_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Optimal Pick Sequence for Robotic Manipulation')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pick_sequence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Pick sequence visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Critical Analysis of Failure Modes (150-200 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critical Analysis: Three Potential Failure Modes in Real-World Deployment**\n",
    "\n",
    "**1. Occlusion and Object Stacking**: The current pipeline assumes all objects are fully visible and separated on a flat surface. In real scenarios, objects often partially occlude each other or are stacked. This causes detection failures where the system might only identify the topmost object or misidentify partially visible objects, leading to incorrect centroid calculations and failed grasp attempts. The robot might try to grasp an object that's actually behind another, resulting in collisions.\n",
    "\n",
    "**2. Lighting and Shadow Variations**: The detection model's performance heavily depends on consistent lighting conditions. Real-world environments have dynamic lighting from windows, overhead lights, and shadows cast by the robotic arm itself. Sudden lighting changes can cause false positives, missed detections, or confidence score fluctuations that disrupt the priority ranking. Shadows might be mistaken for objects or alter the perceived boundaries of actual objects.\n",
    "\n",
    "**3. Grasp Point Accuracy and Object Properties**: Using geometric centroids as grasp points is overly simplistic. Objects have varying mass distributions, surface materials, and shapes that affect optimal grasp locations. A coffee mug's handle makes the centroid unsuitable for grasping. Slippery surfaces, irregular shapes, or fragile objects require specialized grasp strategies not captured by this pipeline. The system lacks force feedback and tactile sensing to adapt when initial grasp attempts fail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2.2: Semantic Segmentation for Autonomous Navigation (2.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Simulated Corridor Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulated indoor corridor scene\n",
    "corridor_height, corridor_width = 600, 800\n",
    "corridor_scene = np.ones((corridor_height, corridor_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "# Define color codes\n",
    "FLOOR_COLOR = (200, 200, 200)  # Gray floor\n",
    "WALL_COLOR = (100, 100, 150)   # Blue-gray walls\n",
    "TARGET_COLOR = (50, 200, 50)   # Green target\n",
    "OBSTACLE_COLOR = (200, 50, 50) # Red obstacle\n",
    "\n",
    "# Draw floor (bottom half)\n",
    "corridor_scene[300:, :] = FLOOR_COLOR\n",
    "\n",
    "# Draw walls\n",
    "cv2.rectangle(corridor_scene, (0, 0), (150, 600), WALL_COLOR, -1)  # Left wall\n",
    "cv2.rectangle(corridor_scene, (650, 0), (800, 600), WALL_COLOR, -1)  # Right wall\n",
    "cv2.rectangle(corridor_scene, (0, 0), (800, 100), WALL_COLOR, -1)  # Top section\n",
    "\n",
    "# Draw target (doorway)\n",
    "cv2.rectangle(corridor_scene, (350, 50), (450, 100), TARGET_COLOR, -1)\n",
    "\n",
    "# Draw obstacles\n",
    "cv2.rectangle(corridor_scene, (200, 350), (280, 450), OBSTACLE_COLOR, -1)\n",
    "cv2.rectangle(corridor_scene, (520, 380), (600, 480), OBSTACLE_COLOR, -1)\n",
    "\n",
    "# Mark robot position\n",
    "robot_x, robot_y = 400, 520\n",
    "cv2.circle(corridor_scene, (robot_x, robot_y), 20, (0, 0, 255), -1)\n",
    "cv2.circle(corridor_scene, (robot_x, robot_y), 20, (0, 0, 0), 3)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(cv2.cvtColor(corridor_scene, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Simulated Indoor Corridor Scene')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('corridor_scene.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Apply Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create semantic segmentation mask\n",
    "segmentation_mask = np.zeros((corridor_height, corridor_width, 3), dtype=np.uint8)\n",
    "\n",
    "# Segment different regions\n",
    "# Floor (Green - Traversable)\n",
    "segmentation_mask[300:, 150:650] = (0, 255, 0)\n",
    "\n",
    "# Walls (Red - Obstacles)\n",
    "segmentation_mask[:, :150] = (255, 0, 0)\n",
    "segmentation_mask[:, 650:] = (255, 0, 0)\n",
    "segmentation_mask[:100, :] = (255, 0, 0)\n",
    "\n",
    "# Target (Blue)\n",
    "segmentation_mask[50:100, 350:450] = (0, 0, 255)\n",
    "\n",
    "# Obstacles (Red)\n",
    "segmentation_mask[350:450, 200:280] = (255, 0, 0)\n",
    "segmentation_mask[380:480, 520:600] = (255, 0, 0)\n",
    "\n",
    "# Add legend\n",
    "legend_height = 150\n",
    "legend_img = np.ones((legend_height, corridor_width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "# Draw legend items\n",
    "legend_items = [\n",
    "    ((0, 255, 0), \"Traversable Floor\"),\n",
    "    ((255, 0, 0), \"Walls/Obstacles\"),\n",
    "    ((0, 0, 255), \"Target Destination\"),\n",
    "]\n",
    "\n",
    "for i, (color, label) in enumerate(legend_items):\n",
    "    y_pos = 30 + i * 40\n",
    "    cv2.rectangle(legend_img, (50, y_pos), (100, y_pos+30), color, -1)\n",
    "    cv2.rectangle(legend_img, (50, y_pos), (100, y_pos+30), (0, 0, 0), 2)\n",
    "    cv2.putText(legend_img, label, (120, y_pos+22),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n",
    "\n",
    "# Combine segmentation and legend\n",
    "combined_seg = np.vstack([segmentation_mask, legend_img])\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(cv2.cvtColor(combined_seg, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Semantic Segmentation of Corridor Scene')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('semantic_segmentation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Semantic segmentation saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Path Planning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple A* path planning\n",
    "from collections import deque\n",
    "\n",
    "# Convert segmentation to binary map (1 = traversable, 0 = obstacle)\n",
    "traversable_map = np.zeros((corridor_height, corridor_width), dtype=np.uint8)\n",
    "traversable_map[300:, 150:650] = 1  # Floor is traversable\n",
    "\n",
    "# Remove obstacle areas\n",
    "traversable_map[350:450, 200:280] = 0\n",
    "traversable_map[380:480, 520:600] = 0\n",
    "\n",
    "# Start and goal positions\n",
    "start = (robot_y, robot_x)  # (row, col)\n",
    "goal = (75, 400)  # Target doorway\n",
    "\n",
    "# Simple BFS path finding\n",
    "def find_path(start, goal, traversable_map):\n",
    "    queue = deque([start])\n",
    "    visited = {start: None}\n",
    "    \n",
    "    directions = [(-1, 0), (1, 0), (0, -1), (0, 1), \n",
    "                  (-1, -1), (-1, 1), (1, -1), (1, 1)]  # 8-directional\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        \n",
    "        if current == goal:\n",
    "            # Reconstruct path\n",
    "            path = []\n",
    "            while current is not None:\n",
    "                path.append(current)\n",
    "                current = visited[current]\n",
    "            return path[::-1]\n",
    "        \n",
    "        for dy, dx in directions:\n",
    "            next_pos = (current[0] + dy, current[1] + dx)\n",
    "            \n",
    "            if (0 <= next_pos[0] < corridor_height and \n",
    "                0 <= next_pos[1] < corridor_width and\n",
    "                traversable_map[next_pos] == 1 and\n",
    "                next_pos not in visited):\n",
    "                \n",
    "                queue.append(next_pos)\n",
    "                visited[next_pos] = current\n",
    "    \n",
    "    return None\n",
    "\n",
    "path = find_path(start, goal, traversable_map)\n",
    "\n",
    "if path:\n",
    "    print(f\"Path found with {len(path)} waypoints\")\n",
    "else:\n",
    "    print(\"No path found!\")\n",
    "\n",
    "# Visualize path on segmentation\n",
    "path_vis = segmentation_mask.copy()\n",
    "\n",
    "# Draw path\n",
    "if path:\n",
    "    for i in range(len(path) - 1):\n",
    "        pt1 = (path[i][1], path[i][0])\n",
    "        pt2 = (path[i+1][1], path[i+1][0])\n",
    "        cv2.line(path_vis, pt1, pt2, (255, 255, 0), 4)  # Yellow path\n",
    "    \n",
    "    # Mark waypoints\n",
    "    for i, (y, x) in enumerate(path):\n",
    "        if i % 10 == 0:  # Draw every 10th waypoint\n",
    "            cv2.circle(path_vis, (x, y), 5, (255, 255, 255), -1)\n",
    "\n",
    "# Mark start and goal\n",
    "cv2.circle(path_vis, (robot_x, robot_y), 20, (0, 0, 255), -1)  # Red robot\n",
    "cv2.circle(path_vis, (robot_x, robot_y), 20, (0, 0, 0), 3)\n",
    "cv2.putText(path_vis, \"START\", (robot_x-30, robot_y-25),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "\n",
    "cv2.circle(path_vis, (400, 75), 15, (255, 255, 0), -1)  # Yellow goal\n",
    "cv2.circle(path_vis, (400, 75), 15, (0, 0, 0), 3)\n",
    "cv2.putText(path_vis, \"GOAL\", (420, 80),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.imshow(cv2.cvtColor(path_vis, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Navigation Path Planning with Obstacle Avoidance')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('navigation_path.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Navigation path visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define Decision-Making Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Navigation Decision-Making Logic (Pseudocode):**\n",
    "\n",
    "```\n",
    "FUNCTION navigate_corridor():\n",
    "    WHILE robot has not reached goal:\n",
    "        // Perception\n",
    "        current_segmentation = get_semantic_segmentation()\n",
    "        robot_position = get_current_position()\n",
    "        \n",
    "        // Obstacle Detection\n",
    "        IF dynamic_obstacle_detected_on_path():\n",
    "            IF obstacle_is_moving_away():\n",
    "                WAIT for 2 seconds\n",
    "                CONTINUE\n",
    "            ELSE IF obstacle_is_stationary():\n",
    "                path = replan_path(robot_position, goal, current_segmentation)\n",
    "                IF path exists:\n",
    "                    follow_path(path)\n",
    "                ELSE:\n",
    "                    STOP and request human assistance\n",
    "            ELSE IF obstacle_is_approaching():\n",
    "                STOP immediately\n",
    "                WAIT until obstacle passes\n",
    "                path = replan_path(robot_position, goal, current_segmentation)\n",
    "        \n",
    "        // Path Following\n",
    "        ELSE:\n",
    "            next_waypoint = get_next_waypoint(path)\n",
    "            move_to(next_waypoint)\n",
    "            \n",
    "            // Periodic re-planning\n",
    "            IF steps_since_last_plan > 10:\n",
    "                verify_path_still_valid()\n",
    "                IF path_blocked():\n",
    "                    path = replan_path(robot_position, goal, current_segmentation)\n",
    "        \n",
    "        // Goal Check\n",
    "        IF distance_to_goal < threshold:\n",
    "            RETURN success\n",
    "    \n",
    "    RETURN reached_goal\n",
    "\n",
    "FUNCTION replan_path(start, goal, segmentation):\n",
    "    // Update traversable map from segmentation\n",
    "    traversable_map = extract_traversable_regions(segmentation)\n",
    "    \n",
    "    // Find new path using A* or similar\n",
    "    new_path = find_path(start, goal, traversable_map)\n",
    "    \n",
    "    RETURN new_path\n",
    "```\n",
    "\n",
    "**Key Decision Rules:**\n",
    "1. **Dynamic Obstacle - Moving Away**: Wait briefly (2s) then continue\n",
    "2. **Dynamic Obstacle - Stationary**: Replan path around obstacle\n",
    "3. **Dynamic Obstacle - Approaching**: Emergency stop, wait for clearance\n",
    "4. **No Valid Path**: Stop and request human intervention\n",
    "5. **Periodic Verification**: Recheck path validity every 10 steps\n",
    "6. **Goal Proximity**: Declare success when within threshold distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Comparative Analysis (150-200 words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Semantic Segmentation over Traditional Methods for Robotic Navigation**\n",
    "\n",
    "Semantic segmentation provides significant advantages over traditional edge detection and feature-based methods for robotic navigation. Unlike edge detection which only identifies boundaries without understanding scene context, semantic segmentation assigns meaningful labels to each pixel, enabling the robot to distinguish between traversable floors, walls, obstacles, and target destinations. This contextual understanding is crucial for safe navigation decisions.\n",
    "\n",
    "Traditional feature-based methods like SIFT or ORB detect keypoints and match them across frames but struggle to provide dense spatial understanding of the environment. They work well for localization but fail to answer critical questions like \"Can I drive through this area?\" Semantic segmentation creates complete spatial maps showing exactly which regions are safe to traverse.\n",
    "\n",
    "Furthermore, semantic segmentation is robust to lighting variations and texture changes that confuse edge detectors. A dark floor and bright floor are both correctly classified as traversable, whereas edge detection might fail on texture-less surfaces. Deep learning-based segmentation models also generalize better across different environments, recognizing floors, walls, and obstacles regardless of their specific appearance.\n",
    "\n",
    "The dense pixel-wise predictions enable precise path planning algorithms that can compute optimal collision-free trajectories. This is particularly valuable in cluttered environments where understanding the complete spatial layout is essential for safe and efficient autonomous navigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Submission\n",
    "\n",
    "This notebook has completed all exercises:\n",
    "\n",
    "**Part 1: Generative Models (5 points)**\n",
    "- ✓ Exercise 1.1: VAE Latent Space Analysis (2.5 points)\n",
    "- ✓ Exercise 1.2: GAN Image Analysis (2.5 points)\n",
    "\n",
    "**Part 2: Vision and Robotics (5 points)**\n",
    "- ✓ Exercise 2.1: Robotic Manipulation Pipeline (2.5 points)\n",
    "- ✓ Exercise 2.2: Semantic Segmentation Navigation (2.5 points)\n",
    "\n",
    "**Generated Files:**\n",
    "- `vae_original_images.png`\n",
    "- `vae_interpolation.png`\n",
    "- `vae_random_samples.png`\n",
    "- `gan_mixed_gallery.png`\n",
    "- `gan_random_vectors.png`\n",
    "- `gan_perturbations.png`\n",
    "- `workspace_scene.png`\n",
    "- `detection_annotated.png`\n",
    "- `pick_sequence.png`\n",
    "- `corridor_scene.png`\n",
    "- `semantic_segmentation.png`\n",
    "- `navigation_path.png`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
